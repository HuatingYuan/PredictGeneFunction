

#' TODO: Calculate measures to evaluate model classification performance
#' 
#' @param score: The predicted score vector calculated from the classification model
#' @param label: The actual label value, the class label must be 1 or 0, and be of numeric type
#' @param threshold: When the predicted score is greater than or equal to this value, it is considered positive
#' @param Beta: The setting value of F.measure.
#'              When Beta is greater than 1, sensitivity is emphasized; 
#'              when Beta is less than 1, precision is emphasized; 
#'              when Beta is equal to 1, sensitivity and precision are equally important.
#'              
#'              
PerformanceMetrics <- function(score, label, threshold=0.5, Beta=1) 
{
	predict.label <- as.numeric(score >=  threshold);
	
	TP <- sum((label == predict.label) & (label == 1));
	FP <- sum((label != predict.label) & (label == 0));
	TN <- sum((label == predict.label) & (label == 0));
	FN <- sum((label != predict.label) & (label == 1));		
	
	#sensitivity/Recall/True Positive rate/hit rate
	sensitivity <- TP/(TP+FN);
	recall = sensitivity;
	tpr = sensitivity
	
	#specificity/True negative rate
	specificity <- TN/(TN+FP);
	
	#False Positive rate
	fpr = FP/(TN+FP)
	
	precision <- TP/(TP+FP);
	negative.predictive.value <- TN/(FN+TN);
	accuracy <- (TP+TN)/(TP+FP+TN+FN);
	
	MCC <- (TP*TN-FP*FN)/(sqrt((TP+FN)*(TN+FP)*(TP+FP)*(TN+FN)))
	
	G.mean <- (sensitivity*specificity)^(1/2);
	
	F.measure <- ((1+Beta^2)*sensitivity*precision)/((Beta^2)*precision+sensitivity);
	
	return(c(TP=TP, FP=FP, TN=TN, FN=FN, 
					sensitivity=sensitivity, 
					specificity=specificity, 
					precision=precision,
					recall=recall,
					tpr=tpr,
					fpr=fpr,
					negative.predictive.value=negative.predictive.value, 
					accuracy=accuracy, 
					MCC=MCC, 
					G.mean=G.mean, 
					F.measure=F.measure))
}
###################################################################################



#' TODO: Calculate measures to evaluate model classification performance. 
#'       The difference with 'PerformanceMetrics' is that each object in 'score' has a weight
#' 
#' @param score: The predicted score vector calculated from the classification model
#' @param label: The actual label value, the class label must be 1 or 0, and be of numeric type
#' @param weights: The weight of each object in 'score'
#' @param threshold: When the predicted score is greater than or equal to this value, it is considered positive
#' @param Beta: The setting value of F.measure.
#'              When Beta is greater than 1, sensitivity is emphasized; 
#'              when Beta is less than 1, precision is emphasized; 
#'              when Beta is equal to 1, sensitivity and precision are equally important.
#'              
PerformanceMetricsUsingWeight <- function(score, label, weights, threshold=0.5, Beta=1) 
{
	predict.label <- as.numeric(score >=  threshold);

	TP <- sum(weights[names(label[(label == predict.label) & (label == 1)])]); 
	FP <- sum(weights[names(label[(label != predict.label) & (label == 0)])]);
	TN <- sum(weights[names(label[(label == predict.label) & (label == 0)])]); 
	FN <- sum(weights[names(label[(label != predict.label) & (label == 1)])]);
	
	
	#sensitivity/Recall/True Positive rate/hit rate
	sensitivity <- TP/(TP+FN);
	recall = sensitivity;
	tpr = sensitivity
	
	#specificity/True negative rate
	specificity <- TN/(TN+FP);
	
	#False Positive rate
	fpr = FP/(TN+FP)
	
	precision <- TP/(TP+FP);
	negative.predictive.value <- TN/(FN+TN);
	accuracy <- (TP+TN)/(TP+FP+TN+FN);
	
	MCC <- (TP*TN-FP*FN)/(sqrt((TP+FN)*(TN+FP)*(TP+FP)*(TN+FN)))
	
	G.mean <- (sensitivity*specificity)^(1/2);
	
	F.measure <- ((1+Beta^2)*sensitivity*precision)/((Beta^2)*precision+sensitivity);
	
	return(c(TP=TP, FP=FP, TN=TN, FN=FN, 
					sensitivity=sensitivity, 
					specificity=specificity, 
					precision=precision,
					recall=recall,
					tpr=tpr,
					fpr=fpr,
					negative.predictive.value=negative.predictive.value, 
					accuracy=accuracy, 
					MCC=MCC, 
					G.mean=G.mean, 
					F.measure=F.measure))
}
#####################################################################################



#' TODO: Calculate the average performance measures for different prediction scores under each threshold 
#'       based on the threshold vector generated by the step size
#' 
#' @param scoreList: A list storing predicted scores
#' @param labelList: A list storing the actual label value
#' @param from: The starting value of threshold
#' @param to : The end value of the threshold
#' @param step: step size
#' @param Beta The setting value of F.measure.
#'              When Beta is greater than 1, sensitivity is emphasized; 
#'              when Beta is less than 1, precision is emphasized; 
#'              when Beta is equal to 1, sensitivity and precision are equally important.
#'              
#'              
MeanPerformanceMetrics <- function(scoreList, labelList, from=1, to=0, step=0.01, Beta=1)
{
	if(length(scoreList) != length(labelList)){
		stop("scoreList与labelList without the same length")
	}
	
#	if(length(scoreList) == 1){
#		stop("length is 1，not using PerformanceMetricsList")
#	}
	
	thresholds = seq(from, to, by=step) 
	
	
	num.element = length(scoreList);
	
	results = sapply(thresholds, function(x){
				temp = sapply(1:num.element, function(i){
							PerformanceMetrics(scoreList[[i]], labelList[[i]], x, Beta)
						})
				temp = rowMeans(temp, na.rm=TRUE);
				return(temp)
			})
	
	return(t(results))
}






